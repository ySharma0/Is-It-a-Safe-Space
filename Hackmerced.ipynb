{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eight-track",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize \n",
    "from nltk import word_tokenize\n",
    "from nltk import PorterStemmer\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import TweetTokenizer\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#ML models, and other tools\n",
    "import torch as torch\n",
    "from torch import nn as nn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "royal-information",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "#     df['tweet_token'],\n",
    "#     df['class'],\n",
    "#     test_size = 0.2,\n",
    "#     random_state = 10,\n",
    "#     stratify=df['class'].values,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "supreme-watts",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['the',\n",
       " ',',\n",
       " '.',\n",
       " 'of',\n",
       " 'to',\n",
       " 'and',\n",
       " 'in',\n",
       " 'a',\n",
       " '\"',\n",
       " \"'s\",\n",
       " 'for',\n",
       " '-',\n",
       " 'that',\n",
       " 'on',\n",
       " 'is',\n",
       " 'was',\n",
       " 'said',\n",
       " 'with',\n",
       " 'he',\n",
       " 'as',\n",
       " 'it',\n",
       " 'by',\n",
       " 'at',\n",
       " '(',\n",
       " ')',\n",
       " 'from',\n",
       " 'his',\n",
       " \"''\",\n",
       " '``',\n",
       " 'an',\n",
       " 'be',\n",
       " 'has',\n",
       " 'are',\n",
       " 'have',\n",
       " 'but',\n",
       " 'were',\n",
       " 'not',\n",
       " 'this',\n",
       " 'who',\n",
       " 'they',\n",
       " 'had',\n",
       " 'i',\n",
       " 'which',\n",
       " 'will',\n",
       " 'their',\n",
       " ':',\n",
       " 'or',\n",
       " 'its',\n",
       " 'one',\n",
       " 'after',\n",
       " 'new',\n",
       " 'been',\n",
       " 'also',\n",
       " 'we',\n",
       " 'would',\n",
       " 'two',\n",
       " 'more',\n",
       " \"'\",\n",
       " 'first',\n",
       " 'about',\n",
       " 'up',\n",
       " 'when',\n",
       " 'year',\n",
       " 'there',\n",
       " 'all',\n",
       " '--',\n",
       " 'out',\n",
       " 'she',\n",
       " 'other',\n",
       " 'people',\n",
       " \"n't\",\n",
       " 'her',\n",
       " 'percent',\n",
       " 'than',\n",
       " 'over',\n",
       " 'into',\n",
       " 'last',\n",
       " 'some',\n",
       " 'government',\n",
       " 'time',\n",
       " '$',\n",
       " 'you',\n",
       " 'years',\n",
       " 'if',\n",
       " 'no',\n",
       " 'world',\n",
       " 'can',\n",
       " 'three',\n",
       " 'do',\n",
       " ';',\n",
       " 'president',\n",
       " 'only',\n",
       " 'state',\n",
       " 'million',\n",
       " 'could',\n",
       " 'us',\n",
       " 'most',\n",
       " '_',\n",
       " 'against',\n",
       " 'u.s.',\n",
       " 'so',\n",
       " 'them',\n",
       " 'what',\n",
       " 'him',\n",
       " 'united',\n",
       " 'during',\n",
       " 'before',\n",
       " 'may',\n",
       " 'since',\n",
       " 'many',\n",
       " 'while',\n",
       " 'where',\n",
       " 'states',\n",
       " 'because',\n",
       " 'now',\n",
       " 'city',\n",
       " 'made',\n",
       " 'like',\n",
       " 'between',\n",
       " 'did',\n",
       " 'just',\n",
       " 'national',\n",
       " 'day',\n",
       " 'country',\n",
       " 'under',\n",
       " 'such',\n",
       " 'second',\n",
       " 'then',\n",
       " 'company',\n",
       " 'group',\n",
       " 'any',\n",
       " 'through',\n",
       " 'china',\n",
       " 'four',\n",
       " 'being',\n",
       " 'down',\n",
       " 'war',\n",
       " 'back',\n",
       " 'off',\n",
       " 'south',\n",
       " 'american',\n",
       " 'minister',\n",
       " 'police',\n",
       " 'well',\n",
       " 'including',\n",
       " 'team',\n",
       " 'international',\n",
       " 'week',\n",
       " 'officials',\n",
       " 'still',\n",
       " 'both',\n",
       " 'even',\n",
       " 'high',\n",
       " 'part',\n",
       " 'told',\n",
       " 'those',\n",
       " 'end',\n",
       " 'former',\n",
       " 'these',\n",
       " 'make',\n",
       " 'billion',\n",
       " 'work',\n",
       " 'our',\n",
       " 'home',\n",
       " 'school',\n",
       " 'party',\n",
       " 'house',\n",
       " 'old',\n",
       " 'later',\n",
       " 'get',\n",
       " 'another',\n",
       " 'tuesday',\n",
       " 'news',\n",
       " 'long',\n",
       " 'five',\n",
       " 'called',\n",
       " '1',\n",
       " 'wednesday',\n",
       " 'military',\n",
       " 'way',\n",
       " 'used',\n",
       " 'much',\n",
       " 'next',\n",
       " 'monday',\n",
       " 'thursday',\n",
       " 'friday',\n",
       " 'game',\n",
       " 'here',\n",
       " '?',\n",
       " 'should',\n",
       " 'take',\n",
       " 'very',\n",
       " 'my',\n",
       " 'north',\n",
       " 'security',\n",
       " 'season',\n",
       " 'york',\n",
       " 'how',\n",
       " 'public',\n",
       " 'early',\n",
       " 'according',\n",
       " 'several',\n",
       " 'court',\n",
       " 'say',\n",
       " 'around',\n",
       " 'foreign',\n",
       " '10',\n",
       " 'until',\n",
       " 'set',\n",
       " 'political',\n",
       " 'says',\n",
       " 'market',\n",
       " 'however',\n",
       " 'family',\n",
       " 'life',\n",
       " 'same',\n",
       " 'general',\n",
       " '–',\n",
       " 'left',\n",
       " 'good',\n",
       " 'top',\n",
       " 'university',\n",
       " 'going',\n",
       " 'number',\n",
       " 'major',\n",
       " 'known',\n",
       " 'points',\n",
       " 'won',\n",
       " 'six',\n",
       " 'month',\n",
       " 'dollars',\n",
       " 'bank',\n",
       " '2',\n",
       " 'iraq',\n",
       " 'use',\n",
       " 'members',\n",
       " 'each',\n",
       " 'area',\n",
       " 'found',\n",
       " 'official',\n",
       " 'sunday',\n",
       " 'place',\n",
       " 'go',\n",
       " 'based',\n",
       " 'among',\n",
       " 'third',\n",
       " 'times',\n",
       " 'took',\n",
       " 'right',\n",
       " 'days',\n",
       " 'local',\n",
       " 'economic',\n",
       " 'countries',\n",
       " 'see',\n",
       " 'best',\n",
       " 'report',\n",
       " 'killed',\n",
       " 'held',\n",
       " 'business',\n",
       " 'west',\n",
       " 'does',\n",
       " 'own',\n",
       " '%',\n",
       " 'came',\n",
       " 'law',\n",
       " 'months',\n",
       " 'women',\n",
       " \"'re\",\n",
       " 'power',\n",
       " 'think',\n",
       " 'service',\n",
       " 'children',\n",
       " 'bush',\n",
       " 'show',\n",
       " '/',\n",
       " 'help',\n",
       " 'chief',\n",
       " 'saturday',\n",
       " 'system',\n",
       " 'john',\n",
       " 'support',\n",
       " 'series',\n",
       " 'play',\n",
       " 'office',\n",
       " 'following',\n",
       " 'me',\n",
       " 'meeting',\n",
       " 'expected',\n",
       " 'late',\n",
       " 'washington',\n",
       " 'games',\n",
       " 'european',\n",
       " 'league',\n",
       " 'reported',\n",
       " 'final',\n",
       " 'added',\n",
       " 'without',\n",
       " 'british',\n",
       " 'white',\n",
       " 'history',\n",
       " 'man',\n",
       " 'men',\n",
       " 'became',\n",
       " 'want',\n",
       " 'march',\n",
       " 'case',\n",
       " 'few',\n",
       " 'run',\n",
       " 'money',\n",
       " 'began',\n",
       " 'open',\n",
       " 'name',\n",
       " 'trade',\n",
       " 'center',\n",
       " '3',\n",
       " 'israel',\n",
       " 'oil',\n",
       " 'too',\n",
       " 'al',\n",
       " 'film',\n",
       " 'win',\n",
       " 'led',\n",
       " 'east',\n",
       " 'central',\n",
       " '20',\n",
       " 'air',\n",
       " 'come',\n",
       " 'chinese',\n",
       " 'town',\n",
       " 'leader',\n",
       " 'army',\n",
       " 'line',\n",
       " 'never',\n",
       " 'little',\n",
       " 'played',\n",
       " 'prime',\n",
       " 'death',\n",
       " 'companies',\n",
       " 'least',\n",
       " 'put',\n",
       " 'forces',\n",
       " 'past',\n",
       " 'de',\n",
       " 'half',\n",
       " 'june',\n",
       " 'saying',\n",
       " 'know',\n",
       " 'federal',\n",
       " 'french',\n",
       " 'peace',\n",
       " 'earlier',\n",
       " 'capital',\n",
       " 'force',\n",
       " 'great',\n",
       " 'union',\n",
       " 'near',\n",
       " 'released',\n",
       " 'small',\n",
       " 'department',\n",
       " 'every',\n",
       " 'health',\n",
       " 'japan',\n",
       " 'head',\n",
       " 'ago',\n",
       " 'night',\n",
       " 'big',\n",
       " 'cup',\n",
       " 'election',\n",
       " 'region',\n",
       " 'director',\n",
       " 'talks',\n",
       " 'program',\n",
       " 'far',\n",
       " 'today',\n",
       " 'statement',\n",
       " 'july',\n",
       " 'although',\n",
       " 'district',\n",
       " 'again',\n",
       " 'born',\n",
       " 'development',\n",
       " 'leaders',\n",
       " 'council',\n",
       " 'close',\n",
       " 'record',\n",
       " 'along',\n",
       " 'county',\n",
       " 'france',\n",
       " 'went',\n",
       " 'point',\n",
       " 'must',\n",
       " 'spokesman',\n",
       " 'your',\n",
       " 'member',\n",
       " 'plan',\n",
       " 'financial',\n",
       " 'april',\n",
       " 'recent',\n",
       " 'campaign',\n",
       " 'become',\n",
       " 'troops',\n",
       " 'whether',\n",
       " 'lost',\n",
       " 'music',\n",
       " '15',\n",
       " 'got',\n",
       " 'israeli',\n",
       " '30',\n",
       " 'need',\n",
       " '4',\n",
       " 'lead',\n",
       " 'already',\n",
       " 'russia',\n",
       " 'though',\n",
       " 'might',\n",
       " 'free',\n",
       " 'hit',\n",
       " 'rights',\n",
       " '11',\n",
       " 'information',\n",
       " 'away',\n",
       " '12',\n",
       " '5',\n",
       " 'others',\n",
       " 'control',\n",
       " 'within',\n",
       " 'large',\n",
       " 'economy',\n",
       " 'press',\n",
       " 'agency',\n",
       " 'water',\n",
       " 'died',\n",
       " 'career',\n",
       " 'making',\n",
       " '...',\n",
       " 'deal',\n",
       " 'attack',\n",
       " 'side',\n",
       " 'seven',\n",
       " 'better',\n",
       " 'less',\n",
       " 'september',\n",
       " 'once',\n",
       " 'clinton',\n",
       " 'main',\n",
       " 'due',\n",
       " 'committee',\n",
       " 'building',\n",
       " 'conference',\n",
       " 'club',\n",
       " 'january',\n",
       " 'decision',\n",
       " 'stock',\n",
       " 'america',\n",
       " 'given',\n",
       " 'give',\n",
       " 'often',\n",
       " 'announced',\n",
       " 'television',\n",
       " 'industry',\n",
       " 'order',\n",
       " 'young',\n",
       " \"'ve\",\n",
       " 'palestinian',\n",
       " 'age',\n",
       " 'start',\n",
       " 'administration',\n",
       " 'russian',\n",
       " 'prices',\n",
       " 'round',\n",
       " 'december',\n",
       " 'nations',\n",
       " \"'m\",\n",
       " 'human',\n",
       " 'india',\n",
       " 'defense',\n",
       " 'asked',\n",
       " 'total',\n",
       " 'october',\n",
       " 'players',\n",
       " 'bill',\n",
       " 'important',\n",
       " 'southern',\n",
       " 'move',\n",
       " 'fire',\n",
       " 'population',\n",
       " 'rose',\n",
       " 'november',\n",
       " 'include',\n",
       " 'further',\n",
       " 'nuclear',\n",
       " 'street',\n",
       " 'taken',\n",
       " 'media',\n",
       " 'different',\n",
       " 'issue',\n",
       " 'received',\n",
       " 'secretary',\n",
       " 'return',\n",
       " 'college',\n",
       " 'working',\n",
       " 'community',\n",
       " 'eight',\n",
       " 'groups',\n",
       " 'despite',\n",
       " 'level',\n",
       " 'largest',\n",
       " 'whose',\n",
       " 'attacks',\n",
       " 'germany',\n",
       " 'august',\n",
       " 'change',\n",
       " 'church',\n",
       " 'nation',\n",
       " 'german',\n",
       " 'station',\n",
       " 'london',\n",
       " 'weeks',\n",
       " 'having',\n",
       " '18',\n",
       " 'research',\n",
       " 'black',\n",
       " 'services',\n",
       " 'story',\n",
       " '6',\n",
       " 'europe',\n",
       " 'sales',\n",
       " 'policy',\n",
       " 'visit',\n",
       " 'northern',\n",
       " 'lot',\n",
       " 'across',\n",
       " 'per',\n",
       " 'current',\n",
       " 'board',\n",
       " 'football',\n",
       " 'ministry',\n",
       " 'workers',\n",
       " 'vote',\n",
       " 'book',\n",
       " 'fell',\n",
       " 'seen',\n",
       " 'role',\n",
       " 'students',\n",
       " 'shares',\n",
       " 'iran',\n",
       " 'process',\n",
       " 'agreement',\n",
       " 'quarter',\n",
       " 'full',\n",
       " 'match',\n",
       " 'started',\n",
       " 'growth',\n",
       " 'yet',\n",
       " 'moved',\n",
       " 'possible',\n",
       " 'western',\n",
       " 'special',\n",
       " '100',\n",
       " 'plans',\n",
       " 'interest',\n",
       " 'behind',\n",
       " 'strong',\n",
       " 'england',\n",
       " 'named',\n",
       " 'food',\n",
       " 'period',\n",
       " 'real',\n",
       " 'authorities',\n",
       " 'car',\n",
       " 'term',\n",
       " 'rate',\n",
       " 'race',\n",
       " 'nearly',\n",
       " 'korea',\n",
       " 'enough',\n",
       " 'site',\n",
       " 'opposition',\n",
       " 'keep',\n",
       " '25',\n",
       " 'call',\n",
       " 'future',\n",
       " 'taking',\n",
       " 'island',\n",
       " '2008',\n",
       " '2006',\n",
       " 'road',\n",
       " 'outside',\n",
       " 'really',\n",
       " 'century',\n",
       " 'democratic',\n",
       " 'almost',\n",
       " 'single',\n",
       " 'share',\n",
       " 'leading',\n",
       " 'trying',\n",
       " 'find',\n",
       " 'album',\n",
       " 'senior',\n",
       " 'minutes',\n",
       " 'together',\n",
       " 'congress',\n",
       " 'index',\n",
       " 'australia',\n",
       " 'results',\n",
       " 'hard',\n",
       " 'hours',\n",
       " 'land',\n",
       " 'action',\n",
       " 'higher',\n",
       " 'field',\n",
       " 'cut',\n",
       " 'coach',\n",
       " 'elections',\n",
       " 'san',\n",
       " 'issues',\n",
       " 'executive',\n",
       " 'february',\n",
       " 'production',\n",
       " 'areas',\n",
       " 'river',\n",
       " 'face',\n",
       " 'using',\n",
       " 'japanese',\n",
       " 'province',\n",
       " 'park',\n",
       " 'price',\n",
       " 'commission',\n",
       " 'california',\n",
       " 'father',\n",
       " 'son',\n",
       " 'education',\n",
       " '7',\n",
       " 'village',\n",
       " 'energy',\n",
       " 'shot',\n",
       " 'short',\n",
       " 'africa',\n",
       " 'key',\n",
       " 'red',\n",
       " 'association',\n",
       " 'average',\n",
       " 'pay',\n",
       " 'exchange',\n",
       " 'eu',\n",
       " 'something',\n",
       " 'gave',\n",
       " 'likely',\n",
       " 'player',\n",
       " 'george',\n",
       " '2007',\n",
       " 'victory',\n",
       " '8',\n",
       " 'low',\n",
       " 'things',\n",
       " '2010',\n",
       " 'pakistan',\n",
       " '14',\n",
       " 'post',\n",
       " 'social',\n",
       " 'continue',\n",
       " 'ever',\n",
       " 'look',\n",
       " 'chairman',\n",
       " 'job',\n",
       " '2000',\n",
       " 'soldiers',\n",
       " 'able',\n",
       " 'parliament',\n",
       " 'front',\n",
       " 'himself',\n",
       " 'problems',\n",
       " 'private',\n",
       " 'lower',\n",
       " 'list',\n",
       " 'built',\n",
       " '13',\n",
       " 'efforts',\n",
       " 'dollar',\n",
       " 'miles',\n",
       " 'included',\n",
       " 'radio',\n",
       " 'live',\n",
       " 'form',\n",
       " 'david',\n",
       " 'african',\n",
       " 'increase',\n",
       " 'reports',\n",
       " 'sent',\n",
       " 'fourth',\n",
       " 'always',\n",
       " 'king',\n",
       " '50',\n",
       " 'tax',\n",
       " 'taiwan',\n",
       " 'britain',\n",
       " '16',\n",
       " 'playing',\n",
       " 'title',\n",
       " 'middle',\n",
       " 'meet',\n",
       " 'global',\n",
       " 'wife',\n",
       " '2009',\n",
       " 'position',\n",
       " 'located',\n",
       " 'clear',\n",
       " 'ahead',\n",
       " '2004',\n",
       " '2005',\n",
       " 'iraqi',\n",
       " 'english',\n",
       " 'result',\n",
       " 'release',\n",
       " 'violence',\n",
       " 'goal',\n",
       " 'project',\n",
       " 'closed',\n",
       " 'border',\n",
       " 'body',\n",
       " 'soon',\n",
       " 'crisis',\n",
       " 'division',\n",
       " '&amp;',\n",
       " 'served',\n",
       " 'tour',\n",
       " 'hospital',\n",
       " 'kong',\n",
       " 'test',\n",
       " 'hong',\n",
       " 'u.n.',\n",
       " 'inc.',\n",
       " 'technology',\n",
       " 'believe',\n",
       " 'organization',\n",
       " 'published',\n",
       " 'weapons',\n",
       " 'agreed',\n",
       " 'why',\n",
       " 'nine',\n",
       " 'summer',\n",
       " 'wanted',\n",
       " 'republican',\n",
       " 'act',\n",
       " 'recently',\n",
       " 'texas',\n",
       " 'course',\n",
       " 'problem',\n",
       " 'senate',\n",
       " 'medical',\n",
       " 'un',\n",
       " 'done',\n",
       " 'reached',\n",
       " 'star',\n",
       " 'continued',\n",
       " 'investors',\n",
       " 'living',\n",
       " 'care',\n",
       " 'signed',\n",
       " '17',\n",
       " 'art',\n",
       " 'provide',\n",
       " 'worked',\n",
       " 'presidential',\n",
       " 'gold',\n",
       " 'obama',\n",
       " 'morning',\n",
       " 'dead',\n",
       " 'opened',\n",
       " \"'ll\",\n",
       " 'event',\n",
       " 'previous',\n",
       " 'cost',\n",
       " 'instead',\n",
       " 'canada',\n",
       " 'band',\n",
       " 'teams',\n",
       " 'daily',\n",
       " '2001',\n",
       " 'available',\n",
       " 'drug',\n",
       " 'coming',\n",
       " '2003',\n",
       " 'investment',\n",
       " '’s',\n",
       " 'michael',\n",
       " 'civil',\n",
       " 'woman',\n",
       " 'training',\n",
       " 'appeared',\n",
       " '9',\n",
       " 'involved',\n",
       " 'indian',\n",
       " 'similar',\n",
       " 'situation',\n",
       " '24',\n",
       " 'los',\n",
       " 'running',\n",
       " 'fighting',\n",
       " 'mark',\n",
       " '40',\n",
       " 'trial',\n",
       " 'hold',\n",
       " 'australian',\n",
       " 'thought',\n",
       " '!',\n",
       " 'study',\n",
       " 'fall',\n",
       " 'mother',\n",
       " 'met',\n",
       " 'relations',\n",
       " 'anti',\n",
       " '2002',\n",
       " 'song',\n",
       " 'popular',\n",
       " 'base',\n",
       " 'tv',\n",
       " 'ground',\n",
       " 'markets',\n",
       " 'ii',\n",
       " 'newspaper',\n",
       " 'staff',\n",
       " 'saw',\n",
       " 'hand',\n",
       " 'hope',\n",
       " 'operations',\n",
       " 'pressure',\n",
       " 'americans',\n",
       " 'eastern',\n",
       " 'st.',\n",
       " 'legal',\n",
       " 'asia',\n",
       " 'budget',\n",
       " 'returned',\n",
       " 'considered',\n",
       " 'love',\n",
       " 'wrote',\n",
       " 'stop',\n",
       " 'fight',\n",
       " 'currently',\n",
       " 'charges',\n",
       " 'try',\n",
       " 'aid',\n",
       " 'ended',\n",
       " 'management',\n",
       " 'brought',\n",
       " 'cases',\n",
       " 'decided',\n",
       " 'failed',\n",
       " 'network',\n",
       " 'works',\n",
       " 'gas',\n",
       " 'turned',\n",
       " 'fact',\n",
       " 'vice',\n",
       " 'ca',\n",
       " 'mexico',\n",
       " 'trading',\n",
       " 'especially',\n",
       " 'reporters',\n",
       " 'afghanistan',\n",
       " 'common',\n",
       " 'looking',\n",
       " 'space',\n",
       " 'rates',\n",
       " 'manager',\n",
       " 'loss',\n",
       " '2011',\n",
       " 'justice',\n",
       " 'thousands',\n",
       " 'james',\n",
       " 'rather',\n",
       " 'fund',\n",
       " 'thing',\n",
       " 'republic',\n",
       " 'opening',\n",
       " 'accused',\n",
       " 'winning',\n",
       " 'scored',\n",
       " 'championship',\n",
       " 'example',\n",
       " 'getting',\n",
       " 'biggest',\n",
       " 'performance',\n",
       " 'sports',\n",
       " '1998',\n",
       " 'let',\n",
       " 'allowed',\n",
       " 'schools',\n",
       " 'means',\n",
       " 'turn',\n",
       " 'leave',\n",
       " 'no.',\n",
       " 'robert',\n",
       " 'personal',\n",
       " 'stocks',\n",
       " 'showed',\n",
       " 'light',\n",
       " 'arrested',\n",
       " 'person',\n",
       " 'either',\n",
       " 'offer',\n",
       " 'majority',\n",
       " 'battle',\n",
       " '19',\n",
       " 'class',\n",
       " 'evidence',\n",
       " 'makes',\n",
       " 'society',\n",
       " 'products',\n",
       " 'regional',\n",
       " 'needed',\n",
       " 'stage',\n",
       " 'am',\n",
       " 'doing',\n",
       " 'families',\n",
       " 'construction',\n",
       " 'various',\n",
       " '1996',\n",
       " 'sold',\n",
       " 'independent',\n",
       " 'kind',\n",
       " 'airport',\n",
       " 'paul',\n",
       " 'judge',\n",
       " 'internet',\n",
       " 'movement',\n",
       " 'room',\n",
       " 'followed',\n",
       " 'original',\n",
       " 'angeles',\n",
       " 'italy',\n",
       " '`',\n",
       " 'data',\n",
       " 'comes',\n",
       " 'parties',\n",
       " 'nothing',\n",
       " 'sea',\n",
       " 'bring',\n",
       " '2012',\n",
       " 'annual',\n",
       " 'officer',\n",
       " 'beijing',\n",
       " 'present',\n",
       " 'remain',\n",
       " 'nato',\n",
       " '1999',\n",
       " '22',\n",
       " 'remains',\n",
       " 'allow',\n",
       " 'florida',\n",
       " 'computer',\n",
       " '21',\n",
       " 'contract',\n",
       " 'coast',\n",
       " 'created',\n",
       " 'demand',\n",
       " 'operation',\n",
       " 'events',\n",
       " 'islamic',\n",
       " 'beat',\n",
       " 'analysts',\n",
       " 'interview',\n",
       " 'helped',\n",
       " 'child',\n",
       " 'probably',\n",
       " 'spent',\n",
       " 'asian',\n",
       " 'effort',\n",
       " 'cooperation',\n",
       " 'shows',\n",
       " 'calls',\n",
       " 'investigation',\n",
       " 'lives',\n",
       " 'video',\n",
       " 'yen',\n",
       " 'runs',\n",
       " 'tried',\n",
       " 'bad',\n",
       " 'described',\n",
       " '1994',\n",
       " 'toward',\n",
       " 'written',\n",
       " 'throughout',\n",
       " 'established',\n",
       " 'mission',\n",
       " 'associated',\n",
       " 'buy',\n",
       " 'growing',\n",
       " 'green',\n",
       " 'forward',\n",
       " 'competition',\n",
       " 'poor',\n",
       " 'latest',\n",
       " 'banks',\n",
       " 'question',\n",
       " '1997',\n",
       " 'prison',\n",
       " 'feel',\n",
       " 'attention',\n",
       " ...]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "#Creating glove to then use for overlapping with our dataset\n",
    "import torchtext\n",
    "\n",
    "#glove is a pre-trained library of vocabulary that has every word given thier corresponding identity number\n",
    "glove = torchtext.vocab.GloVe(name='6B',dim=50)\n",
    "glove.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "emotional-webmaster",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-10-98a6d4f1103e>:8: FutureWarning: The default value of regex will change from True to False in a future version.\n  dataframe.tweet = dataframe.tweet.str.replace('[^\\s\\w]','')\n<ipython-input-10-98a6d4f1103e>:9: FutureWarning: The default value of regex will change from True to False in a future version.\n  dataframe.tweet = dataframe.tweet.str.replace('[^\\s\\w]','')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataframe = pd.read_csv('./labeled_data.csv')\n",
    "dataframe = dataframe[['class', 'tweet']]\n",
    "dataframe = dataframe.replace([0,1,2], ['Hate Speech', 'Offensive Language', 'Other'])\n",
    "\n",
    "\n",
    "dataframe = dataframe.apply(lambda x: x.astype(str).str.lower())\n",
    "dataframe.tweet = dataframe.tweet.str.replace('[^\\s\\w]','')\n",
    "dataframe.tweet = dataframe.tweet.str.replace('[^\\s\\w]','')\n",
    "dataframe.tweet = dataframe.tweet.str.replace('rt', '')\n",
    "# dataframe['tweet_recon'] = dataframe['tweet_token'].apply(lambda x: list(ps.stem(i) for i in x))\n",
    "# dataframe['tweet_recon'] = dataframe['tweet_recon'].apply(lambda x: ' '.join(list(i for i in x if i not in stops)))\n",
    "# tweet_tokenizer = Tokenizer(num_words = 4500, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower = True, split = ' ') \n",
    "# tweet_tokenizer.fit_on_texts(texts = dataframe['tweet_token'])\n",
    "\n",
    "# dataframe['tweet_token'] = tweet_tokenizer.texts_to_sequences(texts = dataframe['tweet_token'])\n",
    "\n",
    "dataframe.to_csv('./new_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "exotic-attraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "class Merger():\n",
    "    \n",
    "    def __init__(self, csv_path, max_length):\n",
    "        self.dataframe = pd.read_csv(csv_path)\n",
    "        self.max_length = max_length\n",
    "        self.label_dict = {\n",
    "            \"hate speech\" : 0,\n",
    "            \"offensive language\" : 1,\n",
    "            \"other\": 2\n",
    "        }\n",
    "        self.dataframe['tweet_token'] = self.dataframe['tweet'].apply(lambda x: word_tokenize(x))\n",
    "        \n",
    "        all_mentioned_words = []\n",
    "        for words in self.dataframe['tweet_token']:\n",
    "            all_mentioned_words += words\n",
    "        frequency = Counter(all_mentioned_words)\n",
    "        self.vocab = torchtext.vocab.Vocab(counter = frequency, min_freq = 25, vectors = glove)\n",
    "        \n",
    "        \n",
    "#         print(self.dataframe.head())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe['tweet'])\n",
    "    \n",
    "    def back_to_text(self, tokens):\n",
    "        text = ''\n",
    "        for tok in tokens:\n",
    "            text += self.vocab.itos[tok] + \" \"\n",
    "        return text\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        label = self.label_dict[self.dataframe['class'][index]]\n",
    "        label = torch.tensor(label)\n",
    "        int_tokens = []\n",
    "        tweet_tokens = self.dataframe['tweet_token'][index]\n",
    "        for token in tweet_tokens:\n",
    "            int_tokens.append(self.vocab[token])\n",
    "        if(len(int_tokens) < self.max_length):\n",
    "            num_to_pad = self.max_length - len(int_tokens)\n",
    "            int_tokens += [0] * num_to_pad\n",
    "        else:\n",
    "            int_tokens = int_tokens[:self.max_length]\n",
    "        int_tokens = torch.tensor(int_tokens)\n",
    "        return(int_tokens, label)\n",
    "        \n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Unnamed: 0  ...                                              tweet\n",
       "0           0  ...    mayasolovely as a woman you shouldnt complai...\n",
       "1           1  ...    mleew17 boy dats coldtyga dwn bad for cuffin...\n",
       "2           2  ...    urkindofbrand dawg  80sbaby4life you ever fu...\n",
       "3           3  ...     c_g_anderson viva_based she look like a tranny\n",
       "4           4  ...    shenikarobes the shit you hear about me migh...\n",
       "\n",
       "[5 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>class</th>\n      <th>tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>other</td>\n      <td>mayasolovely as a woman you shouldnt complai...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>offensive language</td>\n      <td>mleew17 boy dats coldtyga dwn bad for cuffin...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>offensive language</td>\n      <td>urkindofbrand dawg  80sbaby4life you ever fu...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>offensive language</td>\n      <td>c_g_anderson viva_based she look like a tranny</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>offensive language</td>\n      <td>shenikarobes the shit you hear about me migh...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "df = pd.read_csv(\"new_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({'other': 4163, 'offensive language': 19190, 'hate speech': 1430})"
      ]
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "class_counter = Counter(df['class'])\n",
    "class_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.16797804946939435, 0.7743211072105879, 0.05770084332001776]"
      ]
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": [
    "ratios = [x/24783 for x in class_counter.values()]\n",
    "ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(4.6096564977179915, 13.419580419580418)"
      ]
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "ratios[1]/ratios[0], ratios[1]/ratios[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "piano-interest",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "24783"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "dataset = Merger( './new_data.csv', 50)\n",
    "len(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "southeast-iceland",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "indices_list = list(range(0, 24783))\n",
    "train_amount = int(0.8* len(dataset))\n",
    "train_indices = list(range(0, train_amount))\n",
    "test_indices = list(range(train_amount, len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "conscious-birthday",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "test_dataset = Subset(dataset, test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "lyric-proportion",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1f8cd429b20>"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = 32, shuffle = True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = 32)\n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "checked-imagination",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([32, 50]), torch.Size([32]))"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "batched_data, batched_labels = next(iter(train_dataloader))\n",
    "batched_data.shape, batched_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "chicken-uganda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLP_model(nn.Module):\n",
    "    def __init__(self, vocab, emb_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_words = len(vocab)\n",
    "        self.emb_size = emb_size\n",
    "        self.emb = nn.Embedding(self.num_words, self.emb_size)\n",
    "        self.emb.from_pretrained(vocab.vectors)\n",
    "        self.lstm = nn.LSTM(input_size = emb_size, hidden_size = 32, batch_first = True, num_layers = 2,bidirectional= True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lin = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, batch_data):\n",
    "        token_embs = self.emb(batch_data)\n",
    "        outputs, (h_n, c_n) = self.lstm(token_embs)\n",
    "        \n",
    "        last_hidden_state = h_n\n",
    "        last_hidden_state = last_hidden_state.permute(1, 0, 2)\n",
    "        last_hidden_state = last_hidden_state.flatten(start_dim = 1)\n",
    "\n",
    "        last_hidden_state = self.relu(last_hidden_state)\n",
    "        logits = self.lin(last_hidden_state)\n",
    "        \n",
    "        return logits\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "monthly-refund",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "NLP_model(\n",
       "  (emb): Embedding(1183, 50)\n",
       "  (lstm): LSTM(50, 32, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (relu): ReLU()\n",
       "  (lin): Linear(in_features=128, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 111
    }
   ],
   "source": [
    "model = NLP_model(vocab = dataset.vocab, emb_size = 50, num_classes = 3)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "dental-semiconductor",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 112
    }
   ],
   "source": [
    "from torch import optim\n",
    "opt = optim.Adam(model.parameters(), lr=0.001)\n",
    "opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "smart-richmond",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss(torch.tensor([4.61, 1, 5]))\n",
    "# loss_func = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "falling-christmas",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_acc(preds, batched_labels):\n",
    "    predicted_classes = torch.softmax(preds, dim = 1).argmax(dim = 1)\n",
    "\n",
    "    num_correct = (predicted_classes == batched_labels).sum()\n",
    "\n",
    "    acc = num_correct/len(batched_labels)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_classwise_acc(preds, batched_labels):\n",
    "    predicted_classes = torch.softmax(preds, dim = 1).argmax(dim = 1)\n",
    "    pred_0 = (predicted_classes == 0)\n",
    "    pred_1 = (predicted_classes == 1)\n",
    "    pred_2 = (predicted_classes == 2)\n",
    "    label_0 = (batched_labels == 0)\n",
    "    label_1 = (batched_labels == 1)\n",
    "    label_2 = (batched_labels == 2)\n",
    "    num_correct_0 = ((pred_0.long() + label_0.long())==2).sum()\n",
    "    num_correct_1 = ((pred_1.long() + label_1.long())==2).sum()\n",
    "    num_correct_2 = ((pred_2.long() + label_2.long())==2).sum()\n",
    "    num_0 = label_0.sum()\n",
    "    num_1 = label_1.sum()\n",
    "    num_2 = label_2.sum()\n",
    "    if(num_0 == 0):\n",
    "        acc_0 = 1\n",
    "    else:\n",
    "        acc_0 = num_correct_0 / num_0\n",
    "    if(num_1 == 0):\n",
    "        acc_1 = 1\n",
    "    else:\n",
    "        acc_1 = num_correct_1 / num_1\n",
    "    if(num_2 == 0):\n",
    "        acc_2 = 1\n",
    "    else:\n",
    "        acc_2 = num_correct_2 / num_2\n",
    "    \n",
    "\n",
    "    return acc_0,acc_1,acc_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "reliable-guarantee",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--------------------------------------------------\n",
      "Train: loss : 1.0241116285324097, accuracy: 0.5137500166893005 | acc_0: 0.11999999731779099, acc_1: 0.5487442016601562, acc_2: 0.45916667580604553\n",
      "Train: loss : 0.9892950654029846, accuracy: 0.5143749713897705 | acc_0: 0.2199999988079071, acc_1: 0.5534712076187134, acc_2: 0.545175313949585\n",
      "Train: loss : 0.9747843146324158, accuracy: 0.5762500166893005 | acc_0: 0.18000000715255737, acc_1: 0.5936287641525269, acc_2: 0.6985634565353394\n",
      "Train: loss : 0.8745886087417603, accuracy: 0.6631249785423279 | acc_0: 0.03999999910593033, acc_1: 0.6825560927391052, acc_2: 0.8402445316314697\n",
      "Train: loss : 0.7593048214912415, accuracy: 0.7331249713897705 | acc_0: 0.20000000298023224, acc_1: 0.7575266361236572, acc_2: 0.8937143087387085\n",
      "Train: loss : 0.6878086924552917, accuracy: 0.7681249976158142 | acc_0: 0.18000000715255737, acc_1: 0.7924449443817139, acc_2: 0.9007936716079712\n",
      "Train: loss : 0.6566739082336426, accuracy: 0.8012499809265137 | acc_0: 0.11999999731779099, acc_1: 0.8320823907852173, acc_2: 0.9408960938453674\n",
      "Train: loss : 0.6750560998916626, accuracy: 0.8174999952316284 | acc_0: 0.14000000059604645, acc_1: 0.8616816997528076, acc_2: 0.9362301826477051\n",
      "Train: loss : 0.623056173324585, accuracy: 0.8125 | acc_0: 0.1599999964237213, acc_1: 0.8450587391853333, acc_2: 0.9555873274803162\n",
      "Train: loss : 0.5754334330558777, accuracy: 0.8462499976158142 | acc_0: 0.1599999964237213, acc_1: 0.8873597979545593, acc_2: 0.9480952620506287\n",
      "Train: loss : 0.6035417914390564, accuracy: 0.8268749713897705 | acc_0: 0.20666667819023132, acc_1: 0.8686341643333435, acc_2: 0.9573233127593994\n",
      "Train: loss : 0.650358259677887, accuracy: 0.8012499809265137 | acc_0: 0.20000000298023224, acc_1: 0.8289828300476074, acc_2: 0.9212779402732849\n",
      "Test: loss : 0.5448363423347473, accuracy: 0.8630005717277527 | acc_0: 0.22580644488334656, acc_1: 0.8987786173820496, acc_2: 0.9015669226646423\n",
      "--------------------------------------------------\n",
      "Train: loss : 0.5672367811203003, accuracy: 0.8500000238418579 | acc_0: 0.11999999731779099, acc_1: 0.9072480797767639, acc_2: 0.9102937579154968\n",
      "Train: loss : 0.5613343715667725, accuracy: 0.8431249856948853 | acc_0: 0.15000000596046448, acc_1: 0.8782846331596375, acc_2: 0.9829509854316711\n",
      "Train: loss : 0.5820947885513306, accuracy: 0.8293750286102295 | acc_0: 0.2629999816417694, acc_1: 0.8639811873435974, acc_2: 0.943888247013092\n",
      "Train: loss : 0.5246258974075317, accuracy: 0.840624988079071 | acc_0: 0.2749999761581421, acc_1: 0.8734275698661804, acc_2: 0.9624285101890564\n",
      "Train: loss : 0.5035343766212463, accuracy: 0.8525000214576721 | acc_0: 0.3579999804496765, acc_1: 0.8767687082290649, acc_2: 0.9502063989639282\n",
      "Train: loss : 0.5378060936927795, accuracy: 0.8231250047683716 | acc_0: 0.40833336114883423, acc_1: 0.8472004532814026, acc_2: 0.9519445300102234\n",
      "Train: loss : 0.5175838470458984, accuracy: 0.824999988079071 | acc_0: 0.4696667194366455, acc_1: 0.8374500274658203, acc_2: 0.9590873122215271\n",
      "Train: loss : 0.45997655391693115, accuracy: 0.8462499976158142 | acc_0: 0.5063333511352539, acc_1: 0.8528085350990295, acc_2: 0.9830635190010071\n",
      "Train: loss : 0.4753294289112091, accuracy: 0.8293750286102295 | acc_0: 0.5066666603088379, acc_1: 0.8296521902084351, acc_2: 0.9580079913139343\n",
      "Train: loss : 0.5292651057243347, accuracy: 0.8206250071525574 | acc_0: 0.38999995589256287, acc_1: 0.8330621123313904, acc_2: 0.9403571486473083\n",
      "Train: loss : 0.47478243708610535, accuracy: 0.8412500023841858 | acc_0: 0.3696666657924652, acc_1: 0.8452413082122803, acc_2: 0.9784444570541382\n",
      "Train: loss : 0.4299599826335907, accuracy: 0.8612499833106995 | acc_0: 0.4753333628177643, acc_1: 0.8721756935119629, acc_2: 0.9562539458274841\n",
      "Test: loss : 0.4672066569328308, accuracy: 0.8446329236030579 | acc_0: 0.4304301142692566, acc_1: 0.8488686084747314, acc_2: 0.9709163308143616\n",
      "--------------------------------------------------\n",
      "Train: loss : 0.3921128213405609, accuracy: 0.8512499928474426 | acc_0: 0.6483333706855774, acc_1: 0.8443453311920166, acc_2: 0.9825000166893005\n",
      "Train: loss : 0.4269406497478485, accuracy: 0.8631250262260437 | acc_0: 0.5443333387374878, acc_1: 0.8713319897651672, acc_2: 0.9630475640296936\n",
      "Train: loss : 0.434248149394989, accuracy: 0.8568750023841858 | acc_0: 0.5189999938011169, acc_1: 0.8606840372085571, acc_2: 0.9632326364517212\n",
      "Train: loss : 0.43526700139045715, accuracy: 0.8618749976158142 | acc_0: 0.5569523572921753, acc_1: 0.866951048374176, acc_2: 0.9514762163162231\n",
      "Train: loss : 0.45380187034606934, accuracy: 0.8506249785423279 | acc_0: 0.5290000438690186, acc_1: 0.8587208390235901, acc_2: 0.9608333110809326\n",
      "Train: loss : 0.4606911838054657, accuracy: 0.8475000262260437 | acc_0: 0.6050000190734863, acc_1: 0.8523892760276794, acc_2: 0.959261953830719\n",
      "Train: loss : 0.4799574017524719, accuracy: 0.8262500166893005 | acc_0: 0.5740476846694946, acc_1: 0.832339882850647, acc_2: 0.9629048109054565\n",
      "Train: loss : 0.4733249545097351, accuracy: 0.8343750238418579 | acc_0: 0.4999999701976776, acc_1: 0.84503573179245, acc_2: 0.9427778720855713\n",
      "Train: loss : 0.4242578148841858, accuracy: 0.8587499856948853 | acc_0: 0.5690000057220459, acc_1: 0.8634436726570129, acc_2: 0.9654999375343323\n",
      "Train: loss : 0.39986899495124817, accuracy: 0.8587499856948853 | acc_0: 0.5916666984558105, acc_1: 0.8622169494628906, acc_2: 0.9611746072769165\n",
      "Train: loss : 0.38602906465530396, accuracy: 0.8631250262260437 | acc_0: 0.5459999442100525, acc_1: 0.8667271137237549, acc_2: 0.983618974685669\n",
      "Train: loss : 0.4781518578529358, accuracy: 0.8374999761581421 | acc_0: 0.5329999923706055, acc_1: 0.847168505191803, acc_2: 0.955809473991394\n",
      "Test: loss : 0.42091965675354004, accuracy: 0.8783440589904785 | acc_0: 0.48268821835517883, acc_1: 0.8894032835960388, acc_2: 0.9528034925460815\n",
      "--------------------------------------------------\n",
      "Train: loss : 0.3841663599014282, accuracy: 0.8537499904632568 | acc_0: 0.546999990940094, acc_1: 0.8590158224105835, acc_2: 0.9677538871765137\n",
      "Train: loss : 0.38743555545806885, accuracy: 0.8600000143051147 | acc_0: 0.6253333687782288, acc_1: 0.8595007061958313, acc_2: 0.9595000743865967\n",
      "Train: loss : 0.39008769392967224, accuracy: 0.8456249833106995 | acc_0: 0.5543333292007446, acc_1: 0.841321587562561, acc_2: 0.9790000319480896\n",
      "Train: loss : 0.3981238603591919, accuracy: 0.8587499856948853 | acc_0: 0.6850000023841858, acc_1: 0.8555706143379211, acc_2: 0.9516666531562805\n",
      "Train: loss : 0.39435628056526184, accuracy: 0.8581249713897705 | acc_0: 0.5933809876441956, acc_1: 0.8624980449676514, acc_2: 0.9737921953201294\n",
      "Train: loss : 0.43887925148010254, accuracy: 0.8512499928474426 | acc_0: 0.5973333716392517, acc_1: 0.8574580550193787, acc_2: 0.9695714712142944\n",
      "Train: loss : 0.3727598190307617, accuracy: 0.8700000047683716 | acc_0: 0.5529999732971191, acc_1: 0.8765369653701782, acc_2: 0.9701428413391113\n",
      "Train: loss : 0.39846566319465637, accuracy: 0.8568750023841858 | acc_0: 0.5189999938011169, acc_1: 0.8604323863983154, acc_2: 0.9813967943191528\n",
      "Train: loss : 0.3656204640865326, accuracy: 0.8768749833106995 | acc_0: 0.5049999952316284, acc_1: 0.8859707117080688, acc_2: 0.9628332257270813\n",
      "Train: loss : 0.4041200280189514, accuracy: 0.8581249713897705 | acc_0: 0.4546666443347931, acc_1: 0.8683746457099915, acc_2: 0.9728095531463623\n",
      "Train: loss : 0.4009106159210205, accuracy: 0.8575000166893005 | acc_0: 0.6153333187103271, acc_1: 0.858684241771698, acc_2: 0.9768333435058594\n",
      "Train: loss : 0.3827352225780487, accuracy: 0.8712499737739563 | acc_0: 0.5720000267028809, acc_1: 0.8809788227081299, acc_2: 0.9755555987358093\n",
      "Test: loss : 0.428648978471756, accuracy: 0.886790931224823 | acc_0: 0.5350537896156311, acc_1: 0.9008494019508362, acc_2: 0.9305489659309387\n",
      "--------------------------------------------------\n",
      "Train: loss : 0.31826913356781006, accuracy: 0.8706250190734863 | acc_0: 0.6703333258628845, acc_1: 0.8667185306549072, acc_2: 0.9781025052070618\n",
      "Train: loss : 0.36720919609069824, accuracy: 0.8687499761581421 | acc_0: 0.56333327293396, acc_1: 0.8784773945808411, acc_2: 0.9682222008705139\n",
      "Train: loss : 0.35288581252098083, accuracy: 0.8712499737739563 | acc_0: 0.6683333516120911, acc_1: 0.8695794939994812, acc_2: 0.9862856864929199\n",
      "Train: loss : 0.3777083158493042, accuracy: 0.8650000095367432 | acc_0: 0.5730000138282776, acc_1: 0.8683033585548401, acc_2: 0.9742538928985596\n",
      "Train: loss : 0.3787130117416382, accuracy: 0.8575000166893005 | acc_0: 0.5633333325386047, acc_1: 0.8541426658630371, acc_2: 0.9788095355033875\n",
      "Train: loss : 0.34834516048431396, accuracy: 0.8737499713897705 | acc_0: 0.6309999823570251, acc_1: 0.8701654672622681, acc_2: 0.9636666774749756\n",
      "Train: loss : 0.36706870794296265, accuracy: 0.8450000286102295 | acc_0: 0.6459999084472656, acc_1: 0.8365273475646973, acc_2: 0.9829761981964111\n",
      "Train: loss : 0.37159159779548645, accuracy: 0.8525000214576721 | acc_0: 0.6893333196640015, acc_1: 0.8457672595977783, acc_2: 0.9725635647773743\n",
      "Train: loss : 0.38028937578201294, accuracy: 0.8424999713897705 | acc_0: 0.7040951251983643, acc_1: 0.8342154026031494, acc_2: 0.9656189680099487\n",
      "Train: loss : 0.40163907408714294, accuracy: 0.8612499833106995 | acc_0: 0.5950000286102295, acc_1: 0.8577490448951721, acc_2: 0.9644589424133301\n",
      "Train: loss : 0.32523614168167114, accuracy: 0.8762500286102295 | acc_0: 0.6564047336578369, acc_1: 0.8787692785263062, acc_2: 0.9835000038146973\n",
      "Train: loss : 0.37633660435676575, accuracy: 0.8581249713897705 | acc_0: 0.6839999556541443, acc_1: 0.8570676445960999, acc_2: 0.9603333473205566\n",
      "Test: loss : 0.40755462646484375, accuracy: 0.8839892148971558 | acc_0: 0.5227957367897034, acc_1: 0.8944085836410522, acc_2: 0.9461523294448853\n"
     ]
    }
   ],
   "source": [
    "print_rate = 50\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"-\" * 50)\n",
    "    total_loss = 0.0 \n",
    "    total_acc = 0.0\n",
    "    total_acc_0 = 0\n",
    "    total_acc_1 = 0\n",
    "    total_acc_2 = 0\n",
    "    model.train()\n",
    "    for i, (batched_data, batched_labels) in enumerate(train_dataloader,start=1):\n",
    "        preds = model(batched_data)\n",
    "        loss = loss_func(preds, batched_labels)\n",
    "        total_loss+=loss\n",
    "        accuracy = calc_acc(preds, batched_labels)\n",
    "        total_acc+=accuracy\n",
    "        acc_0, acc_1, acc_2 = calc_classwise_acc(preds, batched_labels)\n",
    "        total_acc_0+=acc_0\n",
    "        total_acc_1+=acc_1\n",
    "        total_acc_2+=acc_2\n",
    "        if (i % print_rate == 0):\n",
    "            print(\"Train: loss : {0}, accuracy: {1} | acc_0: {2}, acc_1: {3}, acc_2: {4}\".format(total_loss/print_rate, total_acc/print_rate, total_acc_0/print_rate,total_acc_1/print_rate,total_acc_2/print_rate))\n",
    "            total_loss=0.0\n",
    "            total_acc=0.0\n",
    "            total_acc_0 = 0\n",
    "            total_acc_1 = 0\n",
    "            total_acc_2 = 0\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    total_loss=0.0\n",
    "    total_acc=0.0\n",
    "    total_acc_0 = 0\n",
    "    total_acc_1 = 0\n",
    "    total_acc_2 = 0\n",
    "    model.eval()\n",
    "    for i, (batched_data, batched_labels) in enumerate(test_dataloader):\n",
    "        # print(i)\n",
    "        with torch.no_grad():\n",
    "            preds = model(batched_data)\n",
    "            loss = loss_func(preds, batched_labels)\n",
    "            total_loss+=loss\n",
    "            accuracy = calc_acc(preds, batched_labels)\n",
    "            total_acc+=accuracy\n",
    "            acc_0, acc_1, acc_2 = calc_classwise_acc(preds, batched_labels)\n",
    "            total_acc_0+=acc_0\n",
    "            total_acc_1+=acc_1\n",
    "            total_acc_2+=acc_2\n",
    "    print(\"Test: loss : {0}, accuracy: {1} | acc_0: {2}, acc_1: {3}, acc_2: {4}\".format(total_loss/len(test_dataloader), total_acc/len(test_dataloader), total_acc_0/len(test_dataloader),total_acc_1/len(test_dataloader),total_acc_2/len(test_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "joint-boating",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.state_dict()\n",
    "torch.save(params, 'my_model_weights.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "measured-student",
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model2' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-118-194a201ae750>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mparams_loaded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'my_model_weights.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams_loaded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model2' is not defined"
     ]
    }
   ],
   "source": [
    "params_loaded = torch.load('my_model_weights.pt')\n",
    "model.load_state_dict(params_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "c4d2db86565210e44e1312e025fd2a01c5965d45dad733b18a9ee28031514e1f"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}